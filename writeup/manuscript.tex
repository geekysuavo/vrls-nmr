% \documentclass[preprint,12pt]{elsarticle}
% \documentclass[preprint,review,12pt]{elsarticle}
\documentclass[final,5p,times,twocolumn]{elsarticle}
% ============================================================================

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{url}
\usepackage[
 pdfauthor={Bradley Worley},
 pdftitle={Active Nonuniform Sampling},
 hidelinks]{hyperref}

% ============================================================================
\newcommand{\m}[1]{\boldsymbol{#1}}

\ifpreprint
\newcommand{\halfsize}{0.88\textwidth}
\else
\newcommand{\halfsize}{0.48\textwidth}
\fi

% ============================================================================
\journal{Journal of Magnetic Resonance}

\begin{document}
\begin{frontmatter}

\title{Active Nonuniform Sampling}

\author{Bradley Worley\corref{cor1}}
\ead{geekysuavo@gmail.com}

\affiliation{
 organization={Numerion Labs},
 %addressline={250 Sutter},
 city={San Francisco},
 %postcode={94108},
 state={CA},
 country={USA}
}

% ============================================================================
\begin{abstract}
Nonuniform sampling (NUS) is an effective strategy for reducing experiment
times in multidimensional NMR, enabling practitioners to focus instrument
resources on indirect dimension times that carry maximal information
about the measured signal. Given a suitably chosen sampling schedule,
NUS experiments can yield higher spectral quality in less time, and while
many suitable choices of schedule exist for any given experiment, their
selection is still largely based on rules of thumb. This work examines
the desiderata of \emph{active} nonuniform sampling (ANS), which places a
reconstruction algorithm within the acquisition loop to select new samples
on the fly, and develops a candidate ANS algorithm in order to assess its
performance and practicality in two-dimensional experiments.
\end{abstract}

\begin{graphicalabstract}
\includegraphics[width=1.0\textwidth]{graphical-abstract.pdf}
\end{graphicalabstract}

\begin{highlights}
 \item Variationally reweighted least squares (VRLS) reconstructs
 non-uniformly sampled NMR signals by approximating Bayesian
 posterior predictive distributions.

 \item Active nonuniform sampling (ANS) with VRLS adaptively selects
 optimal nonuniform sampling schedules during acquisition.

 \item ANS selects schedules that minimize VRLS reconstruction
 uncertainty.

 \item Optimal nonuniform sampling schedules depend non-trivially on
 the signal being measured.
\end{highlights}

\begin{keyword}
Active learning \sep
Compressed sensing \sep
Nonuniform sampling \sep
Statistical inference \sep
Variational inference
\end{keyword}

\end{frontmatter}
% \linenumbers

% ============================================================================
\section{Introduction}
\label{sec:intro}
Acquiring high-resolution multidimensional NMR spectra is a crucial yet
highly challenging component of a multitude of modern experimental designs.
Nonuniform sampling (NUS) is a common and powerful strategy
\cite{kazimierczuk:jmr2009,rovnyak:mrc2011,hyberts:jbnmr2013,palmer:jbnmr2014}
to reduce total acquisition time while retaining spectral quality by
measuring part of the signal and computationally estimating the rest
\cite{bretthorst:cmr2008}.
However, NUS is not without its own tradeoffs; the ``reconstruction''
algorithm used to estimate unobserved time-domain data is often non-linear
and may artificially suppress weak signal content, introduce spurious
signal content, or narrow spectral lines \cite{maciejewski:jmr2009}.
Because the quality of reconstruction is sensitive to the choice
of which part of the signal is observed (the ``sampling schedule''), much
effort has been invested in designing sampling schedules that perform well
across the broadest possible range of signals
\cite{hyberts:jacs2010,mobli:jmr2015,worley:jmr2015,worley:jmr2016b,%
cullen:mrc2023,love:jmro2025}.

Unfortunately at present, it would appear that no \emph{a priori} method
exists for selecting an optimal sampling schedule \cite{love:jmro2025}.
The frequencies and amplitudes of artifacts in any given reconstructed NMR
spectrum depend how the schedule interferes with the ground-truth signal: the
act of time-domain subsampling is equivalent to a convolution between the
ground-truth spectrum and the Fourier transform of the schedule, or point
spread function (PSF). For any given signal, the task of reconstruction by
deconvolving the PSF may be helped or hindered by our choice of schedule.
Envelope-matched sampling \cite{schuyler:jbnmr2011} strikes an interesting
balance between total agnosticism and full knowledge of the ground-truth
signal. At the same time, it highlights the central issue of \emph{a priori}
schedule selection: the optimal schedule depends on the signal, and the
signal has yet to be measured.

Active nonuniform sampling (ANS, \cite{worley:cmr2018}) has been proposed as
a principled approach to breaking this dependency cycle. In an ANS
experiment, the choice of sampling schedule is ceded to an algorithm
that progressively refines its estimated reconstruction as data are
observed and proposes grid points to sample next. As ANS aims to maximize
the information gained by each sampled point, proposals are made based on
the current uncertainty of the time-domain signal estimate. This immediately
leads to a set of nontrivial requirements that any ANS algorithm must meet:
\begin{enumerate}
 \item Uncertainty estimates must be computationally inexpensive, as each
  proposal must be ready within the recycle delay.
 \item Uncertainties must be accurate enough for active learning,
  suggesting the use of Bayesian posterior predictive variances.
 \item Running ANS on the same sample multiple times must yield
  quantitatively equivalent results.
\end{enumerate}
As Bayesian methods are not known for speed or determinism
\cite{bretthorst1988,bretthorst:jmr1990a,bretthorst:jmr1990b,%
bretthorst:jmr1990c},
these three requirements are in apparent contradiction. The remainder of this
work uses variational approximation \cite{worley:cmr2018,bishop2007} to build
an algorithm specifically tailored for ANS.

% ============================================================================
\section{Theory}
\label{sec:theory}
The proposed algorithms strongly connect with a broad array of existing
reconstruction methods, making an initial exposition of relevant theory
necessary.

This section begins by reintroducing \emph{iteratively
reweighted least squares} (IRLS) as a classical method for solving sparse
recovery problems in compressed sensing. The machinery of Bayesian inference
is then brought to bear in order to demonstrate that IRLS naturally arises
as the \emph{maximum a posteriori} (MAP) estimator of a particular
probability distribution. The additional machinery of variational
approximation is then employed to construct \emph{variationally reweighted
least squares} (VRLS), which is functionally similar to IRLS but yields
posterior mean and variance estimates. Finally, ANS is developed around
VRLS.

% FIGURE ==========
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-1.pdf}
\caption{%
 (a) $^1$H--$^{15}$N HSQC spectrum reconstructed from 4\%-sparse
 ($m/n=40/1024$) measurements using VRLS. (b) One-dimensional slice taken
 from the reconstruction in (a) overlaid with the square root of its
 accompanying variance estimate.
}
\label{fig:hsqc}
\end{figure*}

\subsection{Sparse recovery}
\label{subsec:sparse}
In compressed sensing, the concept of sparsity is leveraged to estimate
complete signals from incomplete measurements \cite{candes:ieeeit2006,%
donoho:ieeeit2006}. In the context of NUS NMR, the measured data vector
$\m{y}$ is the result of applying the measurement operator $\m{A}$ to
an unknown spectrum $\m{x}$ and adding noise corruptions $\m{\epsilon}$,
i.e.:
\begin{equation}
\m{y} = \m{A} \m{x} + \m{\epsilon},
\label{eq:signalmodel}
\end{equation}
where the $m \times n$ matrix $\m{A} = \m{B} \m{\Phi}$ is the product
of a subsampling matrix $\m{B}$ and an inverse discrete Fourier transform
(iDFT) matrix $\m{\Phi}$. Because $\m{A}$ has fewer rows than columns
($m < n$), we cannot solve \eqref{eq:signalmodel} for $\m{x}$ without
additional information.

To estimate $\m{x}$ despite the limited number of measurements, we include
the assumption that $\m{x}$ is \emph{sparse} (mostly zero) as an additional
source of information. The maximally sparse spectral estimate that agrees
exactly with the measured data is given by the solution to the following
problem:
\begin{equation}
\begin{aligned}
 \underset{\m{x}}{\text{minimize}} &\; \|\m{x}\|_0 \\
 \text{such that} &\; \m{y} = \m{A} \m{x}
\end{aligned}
\label{eq:l0}
\end{equation}
While conceptually appealing, this combinatorial problem is too
computationally expensive to be tractable for even modestly sized $\m{x}$.
Thus, the convex relaxation of \eqref{eq:l0} is preferred:
\begin{equation}
\begin{aligned}
 \underset{\m{x}}{\text{minimize}} &\; \|\m{x}\|_1 \\
 \text{such that} &\; \m{y} = \m{A} \m{x}
\end{aligned}
\label{eq:l1}
\end{equation}
This problem may be efficiently solved by a variety of methods, including
\emph{iterative soft thresholding} (IST, \cite{stern:jmr2007}) and its
accelerated cousin NESTA \cite{becker:siam2011,sun:jbnmr2015}.

The $\ell_1$ norm above is hardly the only sparsity-encoding function at our
disposal. In fact the Hoch-Hore entropy function has a much longer history
of use in NMR data processing \cite{hoch:jmr2017} and has the added benefit
of being smooth \cite{worley:jmr2016a}. Non-convex functions have also been
used with NMR data \cite{kazimierczuk:jmr2011,worley:ieeetsp2019}, but their
use abrogates any existing guarantees that the obtained solution is globally
optimal. Among all sparsing-encoding functions, the $\ell_1$ norm is uniquely
qualified as the ``least convex'' convex function: it encodes sparsity more
strongly than any other convex function.

This work considers the most common special case of NUS NMR, where signals
are acquired in complete quadrature on a uniform (Nyquist) sampling grid
and direct-dimension free induction decays are observed for a subset of
the indirect-dimension grid points based on a sampling schedule. Without
loss of generality, the following equations will focus on the reconstruction
of a single indirect-dimension slice. It is worth mentioning that the common
convention of processing acquired direct-dimension traces followed by
independent reconstructions along the remaining dimensions effectively
ignores nontrivial correlations in the data. However, this practice is
necessary to meet the speed requirement of ANS.

\subsection{Bias-variance tradeoffs}
\label{subsec:bvtrade}
Because an unknown noise corruption $\m{\epsilon}$ was added during the
measurement of $\m{y}$, enforcing exact agreement during reconstruction
is unreasonable, and effectively increases the variance of the estimate. As
is standard practice, we may assume that the noise corruptions are
independently and identically normally distributed with zero mean and
variance $\sigma^2$,
i.e.~$\m{\epsilon} \sim \mathcal{N}(\m{0}, \sigma^2 \m{I})$,
and that $\sigma$ is known exactly or to reasonable precision.
In that case, we may relax the data agreement constraint to yield an
inequality-constrained problem,
\begin{equation}
\begin{aligned}
 \underset{\m{x}}{\text{minimize}} &\; \|\m{x}\|_1 \\
 \text{such that} &\; \| \m{y} - \m{A} \m{x} \|_2^2 \le m \sigma
\end{aligned}
\label{eq:l1_ineq}
\end{equation}
As $\sigma$ is increased, the estimate is biased \emph{away} from the
measured data in favor of decreasing the $\ell_1$ norm, thus reducing
variance. In NUS NMR, this bias-variance tradeoff manifests as a
``linearity vs.~artifact'' tradeoff: low-bias methods that preserve
linearity of spectral integrals can more readily introduce spurious
artifacts than low-variance methods that sacrifice linearity in order
to suppress artifacts.

The problem \eqref{eq:l1_ineq} may be relaxed into an unconstrained
form by introducing a Lagrange multiplier $\zeta$ for its constraint,
\begin{equation}
\underset{\m{x}}{\text{minimize}} \left\{
 \|\m{x}\|_1 + \zeta \, \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\label{eq:l1_uncon}
\end{equation}
where $\zeta$ embodies the bias-variance tradeoff, as it balances the
relative importance of the $\ell_1$ (sparsity) and $\ell_2$ (data fit)
terms in the objective function.

\subsection{Iteratively reweighted least squares}
\label{subsec:irls}
The discontinuity of the $\ell_1$ norm at the origin raises challenges
when constructing minimization algorithms. Successful approaches rely
on incorporating smoothness. For example, IST and NESTA both introduce
strongly convex prox-functions into their objectives
\cite{beck:siam2009,becker:siam2011}.

Another natural approach to incorporating smoothness is by locally
approximating the $\ell_1$ norm by a quadratic function, i.e.:
\begin{equation}
\|\m{x}\|_1 =
 \sum_{i=1}^n |x_i| =
 \sum_{i=1}^n \frac{|x_i|^2}{|x_i|} \approx
 \sum_{i=1}^n \frac{|x_i|^2}{|z_i|} =
 \m{x}^\top \m{W} \m{x}
\label{eq:l1_local_quad}
\end{equation}
where $\m{z}$ is the point around which the approximation is made,
$\m{W}$ is a diagonal matrix containing the weights $1 / |z_i|$.
Substituting this local approximation back into e.g.~\eqref{eq:l1}
results in a simple minimization algorithm composed of two steps per
iteration:
\begin{enumerate}
 \item $\m{x} \gets \m{W}^{-1} \m{A}^\top
        \left( \m{A} \m{W}^{-1} \m{A}^\top \right)^{-1} \m{y}$
 \item $\forall i : W_{ii} \gets |x_i|^{-1}$
\end{enumerate}
which is a form of \emph{iteratively reweighted least squares}
(IRLS, \cite{daubechies:cpam2010,kazimierczuk:jmr2011}).

One particularly serious flaw exists in the above algorithm: if any
$x_i \to 0$, as is often the case in sparse recovery, its accompanying
weight $W_{ii} \to \infty$. To avoid this, the weight update is modified
into $W_{ii} \gets (|x_i|^2 + c)^{-1/2}$, where $c > 0$ is a
small correction term \cite{daubechies:cpam2010}. For suitably chosen
$c$, the modified algorithm still converges to the solution of
\eqref{eq:l1}.

\subsection{Bayesian inference}
\label{subsec:bayes}
Because the quantities of interest in compressed sensing are either unknown
or uncertain, and because ANS specifically requires the estimation of
such uncertainty, it is reasonable to apply the framework of Bayesian
probability theory \cite{ohagan2004}.

We first define the likelihood of observing the data $\m{y}$ from some
known spectrum $\m{x}$. Assuming $\m{\epsilon}$ is independently and
identically normally distributed noise with zero mean and precision
$\tau = 1/\sigma^2$ yields the likelihood,
\begin{equation}
p(\m{y} \mid \m{x}, \tau) =
 (2 \pi)^{-\frac{m}{2}} \tau^{\frac{m}{2}} \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\end{equation}
Placing a normal prior distribution on each spectral value,
\begin{equation}
p(x_i \mid w_i) =
 (2 \pi)^{-\frac{1}{2}} w_i^{\frac{1}{2}} \exp\left\{
  -\frac{1}{2} w_i |x_i|^2
 \right\}
\end{equation}
and an inverse-gamma distribution on the precisions $w_i$ of said prior,
\begin{equation}
p(w_i \mid \xi) =
 \frac{\xi}{2} w_i^{-2} \exp\left\{
  -\frac{\xi}{2} w_i^{-1}
 \right\}
\end{equation}
results in the following conditional distribution,
\begin{equation}
\begin{aligned}
\label{eq:joint}
p(&\m{y}, \m{x}, \m{w} \mid \xi, \tau) \propto \\
 & \left( {\textstyle\prod}_i w_i^{-\frac{3}{2}} \right)
 \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
  -\frac{1}{2} {\textstyle\sum}_i \left( w_i |x_i|^2 + \xi w_i^{-1} \right)
 \right\}
\end{aligned}
\end{equation}
which will be of central importance in subsequent derivations.

By construction, the conditional distribution \eqref{eq:joint} can generate
a broad range of $\ell_1$-IRLS algorithms.\footnote{%
 See the notes in \url{http://github.com/geekysuavo/irls-sandbox}
 for a more complete overview of $\ell_1$-IRLS algorithms derived
 from this joint distribution.}
Moreover, marginalizing $\m{w}$ out of this distribution produces a
familiar distribution,
\begin{equation}
\begin{aligned}
\label{eq:marginal}
p(\m{y}, \m{x} \mid \xi, \tau) &\propto
 \int \mathrm{d}\m{w} p(\m{y}, \m{x}, \m{w} \mid \xi, \tau)
\\ &\propto
 \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
  -\sqrt{\xi} \| \m{x} \|_1
 \right\}
\end{aligned}
\end{equation}
whose \emph{maximum a posteriori} (MAP) estimate of $\m{x}$ corresponds
to the solution of problem \eqref{eq:l1_uncon} with
$\zeta = \tau/(2\sqrt{\xi})$,
\begin{equation}
\begin{aligned}
\m{\hat x}_{\text{MAP}} &=
 \underset{\m{x}}{\arg\max} \; p(\m{x} \mid \m{y}, \xi, \tau)
\\ &=
 \underset{\m{x}}{\arg\max} \; p(\m{y}, \m{x} \mid \xi, \tau)
\\ &=
 \underset{\m{x}}{\arg\min} \left\{
  -\ln p(\m{y}, \m{x} \mid \xi, \tau)
 \right\}
\\ &=
 \underset{\m{x}}{\arg\min} \left\{
  \| \m{x} \|_1 +
  \tfrac{\tau}{2 \sqrt{\xi}} \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\end{aligned}
\end{equation}
While $\m{\hat x}_{\text{MAP}}$ is often a useful point estimate of the
spectrum, ANS requires a more complete description of
$p(\m{x} \mid \m{y}, \xi, \tau)$ than the location of its maximum.
Unfortunately, continuing with a fully Bayesian treatment yields
diminishing returns beyond this point, as the marginal distribution
\eqref{eq:marginal} is not analytically tractable. While Monte Carlo
may be used to estimate the desired quantities, this approach is
computationally expensive, stochastic, and slow to converge.

% FIGURE ==========
\begin{figure}[t]
\centering
\includegraphics[width=\halfsize]{figure-2.pdf}
\caption{%
 On-GPU runtimes of VRLS ($n_{\mathrm{iter}}=100$, black) and
 FMF-VRLS ($n_{\mathrm{iter}}=1000$, red) as a function of measurement
 count $m$, grid point count $n$, and number of parallel reconstructions
 $p$. Times are reported on a logarithmic scale.
}
\label{fig:time}
\end{figure}

\subsection{Variationally reweighted least squares}
\label{subsec:vrls}
We employ variational methods \cite{wainwright:ftml2008} to
approximate the posterior distribution
$p(\m{x}, \m{w} \mid \m{y}, \xi, \tau)$ with a tractable distribution
$q(\m{x}, \m{w})$. To achieve this, we restrict $q$ to the family of
distributions that factorize as follows,
\begin{equation}
\label{eq:family}
\mathcal{Q} = \{ q : q(\m{x}, \m{w}) = q_x(\m{x}) \, q_w(\m{w}) \}
\end{equation}

Given a choice of variational family $\mathcal{Q}$, we find the exact
form of $q$ by minimizing the variational free energy functional,
\begin{equation}
\label{eq:functional}
F(q) =
 -\mathbb{E}_{q(\m{x},\m{w})}[\ln p(\m{y}, \m{x}, \m{w}, \mid \xi, \tau)]
 -\mathbb{H}[q(\m{x},\m{w})]
\end{equation}
over all distributions $q \in \mathcal{Q}$. Formally, this requires the
calculus of variations. However in practice, we may identify $q_x$ and
$q_w$ by taking the logarithm of \eqref{eq:joint} and collecting terms
involving either $\m{x}$ or $\m{w}$, respectively \cite{worley:cmr2018}.
Following this procedure, we see that the minimizing factors $q_x$ and
$q_w$ are a multivariate normal distribution and a product of inverse
normal distributions, respectively,
\begin{align}
q_x(\m{x}) &= \mathcal{N}_n(\m{\mu}, \m{\Gamma})
\\
q_w(w_i) &= \mathcal{IN}(\nu_i, \lambda_i)
\end{align}
making $q(\m{x}, \m{w}) = q_x(\m{x}) \prod_i q_w(w_i)$. With these definitions
in hand, we may rewrite the functional $F(q)$ as a function of the nontrivial
parameters of $q$, namely $\m{\eta} = \{ \m{\mu}, \m{\Gamma}, \m{\nu} \}$,
\begin{equation}
\begin{aligned}
\label{eq:objective}
F(\m{\eta}) &=
 \frac{\tau}{2} \| \m{y} - \m{A} \m{\mu} \|_2^2
+\frac{\tau}{2} \mathrm{tr}(\m{A}^\top \m{A} \m{\Gamma})
-\frac{1}{2} \ln\det\m{\Gamma}
\\ &
+\frac{1}{2} {\textstyle\sum}_i \nu_i (|\mu_i|^2 + \Gamma_{ii})
+\frac{\xi}{2} {\textstyle\sum}_i \nu_i^{-1}
\end{aligned}
\end{equation}
by substituting $q(\m{x}, \m{w})$ into \eqref{eq:functional} and then
minimizing with respect to $\m{\lambda}$, whose optimal value does not
depend on $\m{\eta}$.

The VRLS algorithm is obtained by directly applying alternating minimization
updates to $F(\m{\eta})$. The simplest of these updates is that of the
weights $\m{\nu}$, which is obtained by setting each
$\partial_{\nu_i} F(\m{\eta}) = 0$ and solving for $\nu_i$,
\begin{equation}
\label{eq:nu_update}
\nu_i \gets \sqrt{\xi (|\mu_i|^2 + \Gamma_{ii})^{-1}}
\end{equation}
This mirrors the corrected $\ell_1$-IRLS update defined in
\S\ref{subsec:irls}, but replaces the constant correction term $c$ with
the marginal variance $\Gamma_{ii}$. Thus, VRLS elegantly avoids singularities
by adaptively smoothing the weight update of each spectral point based on its
estimated uncertainty.

To complete an iteration of VRLS, we need to update the remaining parameters
in $\m{\eta}$. However, naively updating either $\m{\mu}$ or $\m{\Gamma}$ in
the same fashion as $\m{\nu}$ will result in expressions involving dense
$n \times n$ matrix inverses,
\begin{equation}
\begin{aligned}
\m{\Gamma} &\gets (\m{V} + \tau \m{A}^\top \m{A})^{-1}
\\
\m{\mu} &\gets \tau \m{\Gamma} \m{A}^\top \m{y}
\end{aligned}
\end{equation}
where $\m{V}$ is a diagonal matrix containing the weights $\m{\nu}$. These
updates are poorly suited to our application, so we will ``kernelize'' them
using the matrix inversion lemma \cite{boyd2004},
\begin{align}
\m{C} &= \m{\Phi} \m{V}^{-1} \m{\Phi}^\top
\\
\label{eq:kernel_update}
\m{K} &= \tau^{-1} \m{I} + \m{B} \m{C} \m{B}^\top
\\
\m{\Gamma} &= \m{V}^{-1} - \m{V}^{-1} \m{A}^\top \m{K}^{-1} \m{A} \m{V}^{-1}
\end{align}
where $\m{K}$ is a much smaller $m \times m$ kernel matrix, and $\m{C}$
is a circulant $n \times n$ matrix whose elements may be computed using a
fast Fourier transform (FFT). With these definitions in hand, the mean
update becomes,
\begin{equation}
\label{eq:mu_update}
\m{\mu} \gets \m{V}^{-1} \m{A}^\top \m{K}^{-1} \m{y}
\end{equation}
which no longer requires the full covariance matrix $\m{\Gamma}$. In fact,
the updates to $\m{\nu}$ only require the diagonal elements of $\m{\Gamma}$,
and the posterior predictive variances of $\m{\hat y}$ only require the
diagonal elements of $\m{\Phi} \m{\Gamma} \m{\Phi}^\top$. Both of these
benefit from kernelization, requiring only $\mathcal{O}(m^2 n)$ operations.
For example, any diagonal element $\Gamma_{ii}$ may be computed using the
matrix inversion lemma,
\begin{equation}
\label{eq:gamma_update}
\Gamma_{ii} \gets
 \nu_i^{-1} - \nu_i^{-2} \m{e}_i^\top \m{A}^\top \m{K}^{-1} \m{A} \m{e}_i
\end{equation}
where $\m{e}_i$ denotes the $i$-th column of the $n \times n$ identity
matrix. Detailed derivations of these computations are provided in the
\nameref{app:si}.

To summarize, VRLS is a globally convergent algorithm that minimizes the
convex objective function \eqref{eq:objective}, with a per-iteration cost
of $\mathcal{O}(n \log n + m^2 n + m^3)$ operations. One iteration of VRLS
involves the following steps:
\begin{enumerate}
 \item Compute $\m{K}$ using equation \eqref{eq:kernel_update}.
 \item Update $\m{\mu}$ using equation \eqref{eq:mu_update}.
 \item Update each $\Gamma_{ii}$ using equation \eqref{eq:gamma_update}.
 \item Update each $\nu_i$ using equation \eqref{eq:nu_update}.
\end{enumerate}
In subsequent sections, we consider modifications to VRLS and develop an
ANS algorithm that uses VRLS as a subroutine.

% FIGURE ==========
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-3.pdf}
\caption{%
 (a) Relative mean-squared error reductions
 $(L_{\mathrm{unif}}-L_{\mathrm{ANS}})/L_{\mathrm{unif}}$ in the
 noise-only regions of VRLS reconstructions produced by uniform
 random NUS and ANS. (b) Fraction of Monte Carlo trials in which
 ANS had lower VRLS reconstruction errors than another \emph{a priori}
 schedule. As $n_{\max}=2048$, undersampling rates of 5\%, 10\% and 20\%
 correspond to $m_{\max}$ values of 102, 205 and 410, respectively.
}
\label{fig:errors}
\end{figure*}

\subsection{Extensions to VRLS}
\label{subsec:extensions}
The decision to leave $q_x$ intact in the variational family $\mathcal{Q}$
is intentional in VRLS. A crucial component of ANS is the posterior
predictive covariance matrix of the complete time-domain signal
$\m{\hat y} = \m{\Phi} \m{x}$, which is equal to
$\m{\Phi} \m{\Gamma} \m{\Phi}^\top$. Had we instead restricted $q$ to
a fully factorized \emph{mean-field} family,
\begin{equation}
\mathcal{Q}_{\mathrm{MF}} =
 \{ q : q(\m{x}, \m{w}) = {\textstyle\prod_i} q_x(x_i) \, q_w(w_i) \}
\end{equation}
the frequency-domain covariance matrix $\m{\Gamma}$ would become diagonal,
and the time-domain covariance matrix  $\m{\Phi} \m{\Gamma} \m{\Phi}^\top$
would become circulant, and therefore
useless in the task of identifying maximally informative grid points.
Nevertheless, the resulting fast mean-field VRLS (FMF-VRLS) algorithm can
be made matrix-free using majorize-minimization techniques
\cite{worley:ieeetsp2019}. The fast mean-field updates to $\m{\mu}$
and $\Gamma_{ii}$ are derived in the \nameref{app:si} to analyze
its performance relative to full VRLS.

As written, the VRLS algorithm depends on \emph{a priori} knowledge of the
noise precision $\tau$ and weight parameter $\xi$. While $\tau$ may be
estimated from a noise sample, and empirical results strongly suggest that
tying $\xi=\tau$ yields the best estimates, the two parameters balance the
data fit and sparsity terms in \eqref{eq:joint}, and may require manual
tuning. Alternatively, we may place inverse-gamma priors on $\tau$ and $\xi$
and extend VRLS to learn approximate posterior factors $q_\tau$ and $q_\xi$.
The extended VRLS objective and updates are derived in the \nameref{app:si}.

\subsection{Active sampling}
\label{subsec:ans}
To complete our ANS algorithm, we use VRLS to select time-domain grid points
that have maximum expected information gain \cite{mackay:nc1992}. In our
variational Bayesian framework, this is equivalent to selecting the grid
points whose posterior predictive variance is largest. Because the VRLS
estimate of the spectrum $\m{x}$ is a normal distribution with mean
$\m{\mu}$ and covariance $\m{\Gamma}$, the complete time-domain signal
$\m{\hat y} = \m{\Phi} \m{x}$ is also normally distributed,
\begin{equation}
q_{\hat y}(\m{\hat y}) =
 \mathcal{N}_n(\m{\Phi} \m{\mu}, \m{\Phi} \m{\Gamma} \m{\Phi}^\top)
\end{equation}
and we need only the marginal variances, i.e.~the diagonal elements
$\Sigma_{ii} = \m{e}_i^\top \m{\Phi} \m{\Gamma} \m{\Phi}^\top \m{e}_i$
of its covariance matrix. As derived in the \nameref{app:si}, this
computation may take advantage of the precomputed $\m{C}$ and
$\m{K}^{-1}$ matrices by way of the matrix inversion lemma:
\begin{equation}
\Sigma_{ii} \gets
 n^{-1} {\textstyle\sum_i} \nu_i^{-1} -
 \m{e}_i^\top \m{C}^\top \m{B}^\top \m{K}^{-1} \m{B} \m{C} \m{e}_i
\end{equation}
which requires only $\mathcal{O}(m^2 n)$ operations and no additional
memory. Thus, in each iteration of ANS, we use VRLS to compute all
marginal variances $\{\Sigma_{ii}\}_{i \in [n]}$, find the grid point
$i_*$ having maximal marginal variance, add $i_*$ into the schedule,
and pass $i_*$ back to the spectrometer for observation. Ideally, this
process occurs within the recycle delay of the experiment and adds no
additional overhead to the total acquisition time.

While VRLS was derived from a probabilistic perspective, it is ultimately
an $\ell_1$-IRLS algorithm subject to compressed sensing (CS) theory
\cite{donoho:ieeeit2006}. As a consequence, ANS must contend with a sort
of ``cold start'' problem: naively beginning with a single measurement
on a large grid places VRLS outside the desirable region of its
sparsity-undersampling phase diagram \cite{donoho:pnas2009} where
successful reconstructions are expected. To overcome this, ANS begins
with an initial number of measurements $m_0$ and a small initial grid
$n_0$. As new measurements are acquired, ANS gradually increases the
grid size $n$ using a CS-inspired heuristic,
\begin{equation}
\label{eq:gridsize}
n_* = \sup\left\{
  n \le n_{\max}
 \mid
  m/n > \varepsilon
 \right\}
\end{equation}
where $\varepsilon \in (0,1)$ is a rough estimate of the sparsity of
the signal, i.e.~the fraction of nonzero elements of $\m{x}$.

% FIGURE ==========
\begin{figure}[t]
\centering
\includegraphics[width=\halfsize]{figure-4.pdf}
\caption{%
 Representative VRLS reconstructions produced from \emph{a priori} NUS
 and ANS simulations of signals with randomly generated parameters.
 From front to back, traces are the noisy ground truth signal,
 the ANS reconstruction, and a VRLS reconstruction of uniform random
 NUS with the same acquisition budget ($m_{\max}=64$, $n_{\max}=2048$).
}
\label{fig:ans}
\end{figure}

% ============================================================================
\section{Materials and methods}
\label{sec:methods}

\subsection{Datasets}
\label{subsec:datasets}
To evaluate the performance of VRLS on realistic NUS data, a
two-dimensional gradient-enhanced $^1$H--$^{15}$N HSQC dataset
that was initially described in \cite{worley:jmr2016a} was reconstructed
from 4\%-sparse quadrature measurements ($m=40, n=1024$). One hundred
iterations of VRLS were performed with $\tau$ and $\xi$ fixed to
$1/\sigma^2$, where $\sigma$ was estimated by manual inspection of
the first transient.

Synthetic data were generated to test ANS across a broad variety of
signal parameters. All simulated signals have the form
\begin{equation}
\label{eq:truth}
y(t) = \sum_{c=1}^{n_c} \alpha_c \exp\left\{
  2 \pi (i \omega_c - \rho_c) t + i \theta_c
 \right\} + \epsilon
\end{equation}
with amplitudes $\{\alpha_c\}$ drawn from a scaled chi-squared distribution
having $\delta_\alpha$ degrees of freedom and mean $\bar\alpha$,
frequencies $\{\omega_c\}$ drawn from a uniform distribution spanning
the entire spectral width, decay rates $\{\rho_c\}$ drawn from a scaled
chi-squared distribution with $\delta_\rho$ degrees of freedom and mean
$\bar\rho$, and phase shifts $\{\theta_c\}$ drawn from a normal
distribution with zero mean and standard deviation $s_\theta$.
The noise $\epsilon$ is drawn from a normal distribution with standard
deviation $\sigma$.

ANS experiments were simulated using synthetic data generated according
to equation \eqref{eq:truth} across a range of signal parameters
(cf.~\nameref{app:si}). In all simulations, one hundred iterations of
VRLS were run after each acquired time point with $\tau$ and $\xi$ fixed
to $1/\sigma^2$. All experiments used $n_{\max}=2048$, corresponding to a
time-domain grid of 1024 points, and $m_{\max}$ was set based on the
desired undersampling rate. For simplicity, $m_0$ was initialized to
$m_{\max}/8$ and $n_0$ to $4 m_0$, which amounts to using 12\% of the
total acquisition budget to measure an initial uniform stretch
\cite{cullen:mrc2023} and a modest 25\% initial undersampling rate
as defined by $m_0 / n_0$.

Paired with each ANS experiment, which produces a schedule and a
corresponding measurement $\m{y}$ from a simulated signal, measurements
were acquired from the same signal with the same acquisition budget using
uniform random sampling, envelope-matched exponential random sampling
\cite{schuyler:jbnmr2011}, and Poisson-gap sampling \cite{hyberts:jacs2010}.
Measurements from each schedule were then reconstructed using VRLS,
FMF-VRLS, and IST-S \cite{stern:jmr2007}. Spectral reconstruction
mean-squared errors (MSE) of the form
\begin{equation}
L(\m{\hat x}; \m{x}) = n_{\max}^{-1} \| \m{x} - \m{\hat x} \|_2^2
\end{equation}
were computed for each combination of schedule and reconstruction algorithm.
Mean-squared errors were computed for entire spectra as well as the spectral
regions known to contain only signal or noise. Relative errors of the form
\begin{equation}
(L_{\mathrm{sched}}-L_{\mathrm{ANS}})/L_{\mathrm{sched}}
\end{equation}
were computed to compare errors of ANS to \emph{a priori}
schedules for the same algorithm, and relative errors of the form
\begin{equation}
(L_{\mathrm{algo}}-L_{\mathrm{VRLS}})/L_{\mathrm{algo}}
\end{equation}
were computed to compare errors of VRLS to other
reconstruction algorithms for the same schedule.

% FIGURE ==========
\begin{figure}[t]
\centering
\includegraphics[width=\halfsize]{figure-5.pdf}
\caption{%
 Distribution of sampling schedules produced from 1000 repeated ANS
 simulations of the same noisy signal ($\sigma=0.01$). The gap in
 sampled grid points in the middle of the grid is a consequence of
 the grid size heuristic, which uses power-of-two grid sizes.
}
\label{fig:sched}
\end{figure}

\subsection{Software implementation}
\label{subsec:impl}
A PyTorch \cite{pytorch} extension module was written in C++14 that
implements the VRLS, extended VRLS, and fast mean-field VRLS algorithms.
Computations of the kernel matrix, frequency-domain marginal variances,
and time-domain marginal variances are performed in parallel using the
OpenMP API \cite{dagum:cse1998} on the CPU and using
CUDA \cite{nickolls:siggraph2008} on the GPU. Accelerating computations
on the GPU is important for minimizing the time required to propose new
grid points for sampling, and PyTorch provides many highly optimized
linear algebra and FFT operators for the GPU, making it an ideal
framework for prototyping an ANS algorithm.

\subsection{Software benchmarking}
\label{subsec:bench}
All benchmarks and analyses were conducted on a 2.6 GHz Intel Xeon
8000-series 32-core machine with 226 GiB RAM and a single NVIDIA A10 GPU
(24 GB). The differing runtimes and convergence rates of standard and
mean-field VRLS were compared on simulated signals to assess suitability
for ANS, and full ANS experiments were simulated in order to analyze the
resulting schedules and determine the conditions in which successful ANS
acquisition is feasible.

% FIGURE ==========
\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{figure-6.pdf}
\caption{%
 Mean and standard deviations ($\mu\pm 3\sigma$) of point spread
 functions for sampling schedules produced from 1000 repeated ANS
 simulations of the same noisy signal at (a) $\sigma=0.01$ and
 (b) $\sigma=0.5$ arb.~units.
}
\label{fig:psf}
\end{figure*}

% ============================================================================
\section{Results and discussion}
\label{sec:results}

\subsection{Reconstruction quality and runtime}
\label{subsec:vrlsresults}
As illustrated by Figure \ref{fig:hsqc}, VRLS is a capable NUS
reconstruction method on its own, given data acquired from a fixed
schedule. While VRLS provides a point estimate of the spectrum in
$\m{\mu}$, it also provides a variance $\Gamma_{ii}$ with each spectral
point $\mu_i$. Because $q_x(\m{x})$ is an approximation of the intractable
exact posterior $p(\m{x} \mid \m{y}, \xi, \tau)$, the variances from VRLS
tend to under-estimate the true posterior variances
\cite{wainwright:ftml2008}. Nevertheless, they are useful for
distinguishing truly recovered signals from artifacts.

When posterior variance estimates are not required, fast mean-field
(FMF) VRLS can provide mean estimates $\m{\mu}$ of equal quality to
VRLS without requiring the storage or inversion of matrices. However,
the matrix-free advantage of FMF-VRLS comes with disadvantages. Because
FMF-VRLS neglects posterior correlations between spectral points, it
dramatically under-estimates frequency domain posterior variance,
and completely fails to usefully estimate time-domain posterior variance.
Moreover, it often requires an order of magnitude more iterations than
VRLS to converge. Figure \ref{fig:time} summarizes this effect: for
sufficiently small $m$ (high undersampling), VRLS requires less time than
FMF-VRLS to converge to an equivalent or better mean estimate.

\subsection{Active nonuniform sampling simulations}
\label{subsec:ansresults}
Comparing the reconstruction mean-squared errors $L(\m{\mu}; \m{x})$
from ANS and static NUS surprisingly reveals equivalent performance.
However, comparing their reconstruction errors in near-resonance and
noise-only spectral regions shows diverging behaviors
(Figures \ref{fig:errors}, \ref{fig:ans}). Near true resonances, ANS
errors are the same or slightly increased relative to static NUS,
suggesting a tendency of ANS to concentrate any artifacts it produces
around real spectral content. In noise-only regions, ANS errors are
dramatically reduced relative to static NUS, which tends to produce
sharper artifacts more randomly within the spectral window. These trends
are mirrored in the posterior variance estimates from ANS and static NUS,
demonstrating that ANS preferentially selects grid points that inform
VRLS about where the signal \emph{is not present} in the spectrum, as
opposed to where it is. Viewed from the lens of solving the highly
underdetermined linear system \eqref{eq:signalmodel}, such a
``process of elimination'' is not an altogether unreasonable strategy.

Perhaps less surprisingly, the degree to which ANS reduces the error
of noise-only spectral regions, relative to static NUS, depends on
the measurement noise $\sigma$ and the final undersampling rate $m/n$
(Figure \ref{fig:errors}). When acquiring noisy data, the information
gained for each new grid point is lower, reducing the advantage of ANS.
Likewise, in less aggressively undersampled experiments, the variability
of reconstructions due to the choice of schedule is diminished. By
leveraging the uncertainty estimates and variational free energies
computed by VRLS to form an early stopping criterion, we may therefore
limit ANS to this ideal undersampling regime of $m \ll n$ while
controlling reconstruction error.

Repeated simulations of ANS using the same ground truth signal with
different samples of noise $\epsilon$ show that resulting schedules
can vary substantially for subtly different measurements
(Figure \ref{fig:sched}). In other words, while VRLS is a
deterministic and globally convergent algorithm, the variances it
produces remain sensitive to measurement noise. This corroborates the
observation in \cite{schuyler:jbnmr2011} that reconstructions from
beat-matched sampling were less robust than envelope-matched sampling,
despite using more \emph{a priori} information. Even still, overall
trends may be observed in the PSFs of schedules produced by repeated
runs of ANS. Figure \ref{fig:psf} shows that ANS systematically
minimizes the PSF of its schedules in a data-dependent manner, an
effect also observed with a different variational method in
\cite{worley:cmr2018}. The size of this effect depends on measurement
noise, with average PSFs from ANS simulations of noisier signals
exhibiting less pronounced, broader minima.

% ============================================================================
\section{Conclusions}
\label{sec:concl}
The development of VRLS in this work takes active nonuniform sampling from
the slow, infeasible theoretical possibility presented in
\cite{worley:cmr2018} and progresses it into a practical methodology.
By carefully constructing a joint density \eqref{eq:joint} that
marginalizes to a Laplace prior on $\m{x}$, VRLS avoids complexities
faced by non-convex alternative algorithms like sparse Bayesian learning
(SBL, \cite{worley:ieeetsp2019}). By retaining just enough structure
in $q(\m{x}, \m{w})$ and leveraging the partial Fourier nature of the
measurement matrix $\m{A}$, VRLS efficiently estimates marginal variances
that enable post-reconstruction quality checking and active sampling.
VRLS is well-suited to GPU acceleration due to the embarrassingly parallel
nature of marginal variance estimation; planned optimizations to the CUDA
implementation are expected to further improve runtime scaling with $m$.

Certain characteristics of ANS still warrant further examination.
For example, refinement of the grid size heuristic \eqref{eq:gridsize}
could nudge the algorithm towards producing sampling schedules with more
homogenous time-domain coverage, even though it remains to be seen whether
this is more than an aesthetic trait. Perhaps most obviously, further work
is needed to extend VRLS to multidimensional data where the advantages of
NUS are greater. Doing so is largely an engineering exercise that will
require careful indexing and multi-complex FFTs, as nothing in VRLS
precludes its generalization to higher dimensionalities.

To the author's knowledge, VRLS is the first algorithm to satisfy the
desiderata of ANS and consequently enable time-efficient model-based
active learning on NMR spectrometers.\footnote{%
 Aside from the variational Bayesian approach to ANS presented in
 \cite{worley:cmr2018}, other authors have attempted ANS using the
 sample variance of diffusion models \cite{goffinet:arxiv2025}.
 However, the models in \cite{goffinet:arxiv2025} require minutes to
 propose new grid points and suffer from all the shortcomings of deep
 learning (black box inscrutability, unclear out-of-domain generalization,
 inference-time non-determinism, etc.).
} However, it remains limited in
many important aspects. Principal among these limitations its its continued
use of a fixed-basis signal model \eqref{eq:signalmodel}, which discards
nearly all available prior information about the ground truth signal in
exchange for the use of a fast Fourier transform and a downstream cascade
of signal processing, peak picking, and so on. Prior knowledge of
experimental parameters, the number of resonances and their relationships
in protein samples, or the contributions of possible analytes in complex
mixtures, are all prime candidates for Bayesian modeling. While such
parametric signal models are currently too slow for ANS, they promise
the dual advantages of (i) further reducing acquisition time by leveraging
prior information and (ii) providing spectroscopists with richer and more
immediate experimental results. Indeed, further work is needed to develop
Bayesian NMR signal models that make full use of available prior knowledge
while also admitting active learning.

% ============================================================================
\appendix
\section{Supplementary information}
\label{app:si}
This material is available free of charge via the Internet at
\url{http://www.sciencedirect.com}. Supplementary data associated
with this article can be found, in the online version, at
\url{http://dx.doi.org/10.1016/j.jmr.0000.00.000}

\section*{Data availability}
\label{app:data}
The data and code used to produce this work can be found online at
\url{http://github.com/geekysuavo/vrls-nmr}.

% ============================================================================
\bibliographystyle{elsarticle-num}
\bibliography{vrls}

\end{document}
\endinput
