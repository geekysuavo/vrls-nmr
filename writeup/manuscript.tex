% \documentclass[preprint,12pt]{elsarticle}
% \documentclass[preprint,review,12pt]{elsarticle}
\documentclass[final,5p,times,twocolumn]{elsarticle}
% ============================================================================

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{url}
\usepackage[
 pdfauthor={Bradley Worley},
 pdftitle={Active Nonuniform Sampling},
 hidelinks]{hyperref}

% ============================================================================
\newcommand{\m}[1]{\boldsymbol{#1}}

% ============================================================================
\journal{Journal of Magnetic Resonance}

\begin{document}
\begin{frontmatter}

\title{Active Nonuniform Sampling}

\author{Bradley Worley\corref{cor1}}
\ead{geekysuavo@gmail.com}

\affiliation{
 organization={Numerion Labs},
 %addressline={250 Sutter},
 city={San Francisco},
 %postcode={94108},
 state={CA},
 country={USA}
}

% ============================================================================
\begin{abstract}
Nonuniform sampling (NUS) is an effective strategy for reducing experiment
times in multidimensional NMR, enabling practitioners to focus instrument
resources on indirect dimension times that carry maximal information
about the measured signal. Given a suitably chosen sampling schedule,
NUS experiments can yield higher spectral quality in less time, and while
many suitable choices of schedule exist for any given experiment, their
selection is still largely based on rules of thumb. This work examines
the desiderata of \emph{active} nonuniform sampling (ANS), which places a
reconstruction algorithm within the acquisition loop to select new samples
on the fly, and develops a candidate ANS algorithm in order to assess its
performance and practicality in two-dimensional experiments.
\end{abstract}

% \begin{graphicalabstract}
% \includegraphics{grabs}
% \end{graphicalabstract}

\begin{keyword}
Active learning \sep
Compressed sensing \sep
Nonuniform sampling \sep
Statistical inference \sep
Variational inference
\end{keyword}

\end{frontmatter}
% \linenumbers

% ============================================================================
\section{Introduction}
\label{sec:intro}
Acquiring high-resolution multidimensional NMR spectra is a crucial yet
highly challenging component of a multitude of modern experimental designs.
Nonuniform sampling (NUS) is a common and powerful strategy
\cite{kazimierczuk:jmr2009,rovnyak:mrc2011,hyberts:jbnmr2013,palmer:jbnmr2014}
to reduce total acquisition time while retaining spectral quality by
measuring part of the signal and computationally estimating the rest
\cite{bretthorst:cmr2008}.
However, NUS is not without its own tradeoffs; the ``reconstruction''
algorithm used to estimate unobserved time-domain data is often non-linear
and may artificially suppress weak signal content, introduce spurious
signal content, or narrow spectral lines \cite{maciejewski:jmr2009}.
Because the quality of reconstruction is sensitive to the choice
of which part of the signal is observed (the ``sampling schedule''), much
effort has been invested in designing sampling schedules that perform well
across the broadest possible range of signals
\cite{hyberts:jacs2010,mobli:jmr2015,worley:jmr2015,worley:jmr2016b,%
cullen:mrc2023,love:jmro2025}.

Unfortunately at present, it would appear that no \emph{a priori} method
exists for selecting an optimal sampling schedule \cite{love:jmro2025}.
The frequencies and amplitudes of artifacts in any given reconstructed NMR
spectrum depend how the schedule interferes with the ground-truth signal: the
act of time-domain subsampling is equivalent to a convolution between the
ground-truth spectrum and the Fourier transform of the schedule, or point
spread function (PSF). For any given signal, the task of reconstruction by
deconvolving the PSF may be helped or hindered by our choice of schedule.
Envelope-matched sampling \cite{schuyler:jbnmr2011} strikes an interesting
balance between total agnosticism and full knowledge of the ground-truth
signal. At the same time, it highlights the central issue of \emph{a priori}
schedule selection: the optimal schedule depends on the signal, and the
signal has yet to be measured.

Active nonuniform sampling (ANS, \cite{worley:cmr2018}) has been proposed as
a principled approach to breaking this dependency cycle. In an ANS
experiment, the choice of sampling schedule is ceded to an algorithm
that progressively refines its estimated reconstruction as data are
observed and proposes grid points to sample next. As ANS aims to maximize
the information gained by each sampled point, proposals are made based on
the current uncertainty of the time-domain signal estimate. This immediately
leads to a set of nontrivial requirements that any ANS algorithm must meet:
\begin{enumerate}
 \item Uncertainty estimates must be computationally inexpensive, as each
  proposal must be ready within the recycle delay.
 \item Uncertainties must be accurate enough for active learning,
  suggesting the use of Bayesian posterior predictive variances.
 \item Running ANS on the same sample multiple times must yield
  quantitatively equivalent results.
\end{enumerate}
As Bayesian methods are not known for speed or determinism
\cite{bretthorst1988,bretthorst:jmr1990a,bretthorst:jmr1990b,%
bretthorst:jmr1990c},
these three requirements are in apparent contradiction. The remainder of this
work uses variational approximation \cite{worley:cmr2018,bishop2007} to build
an algorithm specifically tailored for ANS.

% ============================================================================
\section{Theory}
\label{sec:theory}
This work considers the most common special case of NUS, where signals are
acquired in complete quadrature on a uniform (Nyquist) sampling grid and
direct-dimension free induction decays are observed for a subset of the
indirect-dimension grid points based on a sampling schedule.
Without loss of generality, the following equations
will focus on the reconstruction of a single indirect-dimension slice. It is
worth mentioning that the established convention of processing acquired
direct-dimension traces followed by independent reconstructions along the
remaining dimensions effectively ignores nontrivial correlations in the data.
However, this practice is necessary to meet the speed requirement of ANS.

\subsection{Probabilistic construction}
\label{subsec:bayes}
We begin with the standard fixed-basis \cite{worley:cmr2018}
measurement model,
\begin{equation}
\boldsymbol{y} = \m{A} \m{x} + \m{\epsilon}
\end{equation}
where $\m{y}$ are the observed time-domain data, $\m{x}$ is the spectrum
to be estimated, and $\m{\epsilon}$ is independently and identically
normally distributed noise with zero mean and precision $\tau$. The
$m \times n$ matrix $\m{A} = \m{B} \m{\Phi}$ is composed of a subsampling
matrix $\m{B}$ and a discrete Fourier transform (DFT) matrix $\m{\Phi}$.
Putting these together yields the likelihood,
\begin{equation}
p(\m{y} \mid \m{x}, \tau) =
 (2 \pi)^{-\frac{m}{2}} \tau^{\frac{m}{2}} \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\end{equation}
Placing a normal prior distribution on each spectral value,
\begin{equation}
p(x_i \mid w_i) =
 (2 \pi)^{-\frac{1}{2}} w_i^{\frac{1}{2}} \exp\left\{
  -\frac{1}{2} w_i x_i^2
 \right\}
\end{equation}
and an inverse-gamma distribution on the precisions $w_i$ of said prior,
\begin{equation}
p(w_i \mid \xi) =
 \frac{\xi}{2} w_i^{-2} \exp\left\{
  -\frac{\xi}{2} w_i^{-1}
 \right\}
\end{equation}
results in a conditional distribution,
\begin{equation}
\begin{aligned}
\label{eq:joint}
p(&\m{y}, \m{x}, \m{w} \mid \xi, \tau) \propto \\
 & \left( {\textstyle\prod}_i w_i^{-\frac{3}{2}} \right)
 \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
  -\frac{1}{2} {\textstyle\sum}_i \left( w_i x_i^2 + \xi w_i^{-1} \right)
 \right\}
\end{aligned}
\end{equation}
which will be of central importance in subsequent derivations. While it does
not appear related to other sparsity-inducing priors, this distribution can
generate a broad range of $\ell_1$ iteratively reweighted least squares
(IRLS, \cite{daubechies:cpam2010,kazimierczuk:jmr2011})
algorithms.\footnote{%
 See the notes in \url{http://github.com/geekysuavo/irls-sandbox}
 for a more complete overview of $\ell_1$-IRLS algorithms derived
 from this joint distribution.}

Continuing with a fully Bayesian treatment yields diminishing returns beyond
this point, so we employ variational inference \cite{wainwright:ftml2008} to
approximate the posterior over $\m{x}$ and $\m{w}$ by a distribution of the
form $q(\m{x}, \m{w}) = q_x(\m{x}) \, q_w(\m{w})$ that minimizes the
variational free energy,
\begin{equation}
F(q) =
 -\mathbb{E}_{q(\m{x},\m{w})}[\ln p(\m{y}, \m{x}, \m{w}, \mid \xi, \tau)]
 -\mathbb{H}[q(\m{x},\m{w})]
\end{equation}
Noting that the minimizing factors $q_x$ and $q_w$ are a multivariate
normal distribution and a product of inverse normal distributions,
respectively,
\begin{align}
q_x(\m{x}) &= \mathcal{N}(\m{\mu}, \m{\Gamma})
\\
q_w(w_i) &= \mathcal{IN}(\nu_i, \lambda_i)
\end{align}
we rewrite $F(q)$ as a function of the nontrivial parameters of $q$,
namely $\m{\eta} = \{ \m{\mu}, \m{\Gamma}, \m{\nu} \}$,
\begin{equation}
\begin{aligned}
F(\m{\eta}) &=
 \frac{\tau}{2} \| \m{y} - \m{A} \m{\mu} \|_2^2
+\frac{\tau}{2} \mathrm{tr}(\m{A}^\top \m{A} \m{\Gamma})
-\frac{1}{2} \ln\det\m{\Gamma}
\\ &
+\frac{1}{2} {\textstyle\sum}_i \nu_i (\mu_i^2 + \Gamma_{ii})
+\frac{\xi}{2} {\textstyle\sum}_i \nu_i^{-1}
\end{aligned}
\end{equation}

The decision to leave $q_x$ intact in the variational family is intentional.
A crucial component of ANS is the posterior predictive covariance of the
complete signal $\m{\hat y}$, which is equal to
$\m{\Phi} \m{\Gamma} \m{\Phi}^\top$. Had we sought a fully mean-field
approximation in which $\m{\Gamma}$ is diagonal, the matrix
$\m{\Phi} \m{\Gamma} \m{\Phi}^\top$ would be circulant, and therefore
useless in the task of identifying maximally informative grid points.

\subsection{Variationally reweighted least squares}
\label{subsec:vrls}
The VRLS algorithm is obtained by directly applying alternating minimization
to $F(\m{\eta})$. The simplest of these updates is that of the weights
$\m{\nu}$, which is obtained by setting $\partial\nu_i = 0$ and solving,
\begin{equation}
\nu_i \gets \sqrt{\xi (\mu_i^2 + \Gamma_{ii})^{-1}}
\end{equation}
which mirrors the $\ell_1$-IRLS update of $w_i \gets |x_i|^{-1}$ with
several interesting advantages. In IRLS, a small correction term $c$ is
often added to the update to obtain $w_i \gets (x_i^2 + c)^{-1/2}$ in order
to avoid singularities as $x_i \to 0$ \cite{daubechies:cpam2010}. VRLS
elegantly uses the marginal variance $\Gamma_{ii}$ as an adaptive
correction.

Naively updating either $\m{\mu}$ or $\m{\Gamma}$ in the same fashion
will result in expressions involving dense $n \times n$ matrix inverses,
\begin{equation}
\begin{aligned}
\m{\Gamma} &\gets (\m{V} + \tau \m{A}^\top \m{A})^{-1}
\\
\m{\mu} &\gets \tau \m{\Gamma} \m{A}^\top \m{y}
\end{aligned}
\end{equation}
where $\m{V}$ is a diagonal matrix containing the weights $\m{\nu}$. These
updates are poorly suited to our application, so we will ``kernelize'' them
using the matrix inversion lemma \cite{boyd2004},
\begin{align}
\m{C} &= \m{\Phi} \m{V}^{-1} \m{\Phi}^\top
\\
\m{K} &= \tau^{-1} \m{I} + \m{B} \m{C} \m{B}^\top
\\
\m{\Gamma} &= \m{V}^{-1} - \m{V}^{-1} \m{A}^\top \m{K}^{-1} \m{A} \m{V}^{-1}
\end{align}
where $\m{K}$ is a much smaller $m \times m$ kernel matrix, and $\m{C}$
is a circulant $n \times n$ matrix whose elements may be computed using a
fast Fourier transform (FFT). With these definitions in hand, the mean
update becomes,
\begin{equation}
\m{\mu} \gets \m{V}^{-1} \m{A}^\top \m{K}^{-1} \m{y}
\end{equation}
which no longer requires the full covariance matrix $\m{\Gamma}$. In fact,
the updates to $\m{\nu}$ only require the diagonal elements of $\m{\Gamma}$,
and the posterior predictive variances of $\m{\hat y}$ only require the
diagonal elements of $\m{\Phi} \m{\Gamma} \m{\Phi}^\top$. Both of these
benefit from kernelization, requiring only $\mathcal{O}(m^2 n)$ operations.
Detailed derivations of these computations are provided in the
\nameref{app:si}.

\subsection{Extensions to VRLS}
\label{subsec:extensions}
While VRLS only requires handling a small $m \times m$ kernel matrix, it
is nevertheless matrix-bound. A matrix-free fast mean-field variant of
VRLS may be easily constructed using majorize-minimization techniques
\cite{worley:ieeetsp2019}. Because it forms a mean-field approximation
of the posterior, it cannot be used in ANS. Even so, we derive the fast
mean-field updates to $\m{\mu}$ and $\Gamma_{ii}$ in the \nameref{app:si}
to analyze its performance relative to full VRLS.

As written, the VRLS algorithm depends on \emph{a priori} knowledge of the
noise precision $\tau$ and weight parameter $\xi$. While $\tau$ may be
estimated from a noise sample, and empirical results strongly suggest that
tying $\xi=\tau$ yields the best results, the two parameters balance the
data fit and sparsity terms in \eqref{eq:joint}, and may require manual
tuning. Alternatively, we may place inverse-gamma priors on $\tau$ and $\xi$
and extend VRLS to learn approximate posterior factors $q_\tau$ and $q_\xi$.
The extended VRLS objective and updates are derived in the \nameref{app:si}.

\subsection{Active sampling}
\label{subsec:ans}
To complete our ANS algorithm, we use VRLS to select time-domain grid points
that have maximum expected information gain \cite{mackay:nc1992}. In our
variational Bayesian framework, this is equivalent to selecting the grid
points whose posterior predictive variance is largest. The complete
time-domain signal $\m{\hat y} = \m{\Phi} \m{x}$ has the following
mean and covariance:
\begin{align}
\mathbb{E}_{q(\m{x},\m{w})}[\m{\hat y}] &=
 \m{\Phi} \m{\mu}
\\
\mathrm{Cov}_{q(\m{x},\m{w})}[\m{\hat y}, \m{\hat y}] &=
 \m{\Phi} \m{\Gamma} \m{\Phi}^\top
\end{align}
so we compute the marginal variances and select a new grid point $i_*$
\begin{equation}
i_* = \underset{i \in [n]}{\arg\max}
 \left( \m{e}_i^\top \m{\Phi} \m{\Gamma} \m{\Phi}^\top \m{e}_i \right)
\end{equation}
for observation. Ideally, this process occurs within the recycle delay of
the experiment and adds no additional overhead to the total acquisition
time.

% ============================================================================
\section{Materials and methods}
\label{sec:methods}

\subsection{Software implementation}
\label{subsec:impl}
A PyTorch \cite{pytorch} extension module was written in C++14 that
implements the VRLS, extended VRLS, and fast mean-field VRLS algorithms.
Computations of the kernel matrix, frequency-domain marginal variances,
and time-domain marginal variances are performed in parallel using the
OpenMP API \cite{dagum:cse1998} on the CPU and using
CUDA \cite{nickolls:siggraph2008} on the GPU. Accelerating computations
on the GPU is important for minimizing the time required to propose new
grid points for sampling, and PyTorch provides many highly optimized
linear algebra and FFT operators for the GPU, making it an ideal
framework for an ANS algorithm.

\subsection{Software benchmarking}
\label{subsec:bench}
% begin notes to delete
Please fix me.
\begin{itemize}
\item Reconstructions of my 5\% NUS 1H-15N HSQC
\item Compare quality and convergence rate of VRLS to VRLS-FMF
\item Analyze runtime as $m$ increases for selected fixed $n$
\item Simulate ANS using Micah's US 1H-13C HSQC
\item Simulate ANS using synthetic data
\item Look at PSFs from synthetic ANS runs on same signal
\item Look at $\ell_2$ error and $F$ rates--better than unif./exp.~bias?
\item Should we include an exponential bias in the variance?
\end{itemize}
% end notes to delete

\subsection{Datasets}
\label{subsec:datasets}
Please fix me.

% ============================================================================
\section{Results}
\label{sec:results}
Please fix me.


%\begin{table}[t]
%\centering
%\begin{tabular}{l c r}
%  1 & 2 & 3 \\
%  4 & 5 & 6 \\
%  7 & 8 & 9 \\
%\end{tabular}
%\caption{Table Caption}\label{fig1}
%\end{table}

%\begin{figure}[t]
%\centering
%\includegraphics[width=0.48\textwidth]{example-image-a}
%\caption{Figure Caption}\label{fig1}
%\end{figure}

% ============================================================================
\section{Conclusions}
\label{sec:concl}
Please fix me.

% ============================================================================
\appendix
\section{Supplementary information}
\label{app:si}
This material is available free of charge via the Internet at
\url{http://www.sciencedirect.com}. Supplementary data associated
with this article can be found, in the online version, at
\url{http://dx.doi.org/10.1016/j.jmr.0000.00.000}

\section*{Data availability}
\label{app:data}
The data and code used to produce this work can be found online at
\url{http://github.com/geekysuavo/vrls-nmr}.

% ============================================================================
\bibliographystyle{elsarticle-num}
\bibliography{vrls}

\end{document}
\endinput
