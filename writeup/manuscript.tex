% \documentclass[preprint,12pt]{elsarticle}
% \documentclass[preprint,review,12pt]{elsarticle}
\documentclass[final,5p,times,twocolumn]{elsarticle}
% ============================================================================

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{url}
\usepackage[
 pdfauthor={Bradley Worley},
 pdftitle={Active Nonuniform Sampling},
 hidelinks]{hyperref}

% ============================================================================
\newcommand{\m}[1]{\boldsymbol{#1}}

% ============================================================================
\journal{Journal of Magnetic Resonance}

\begin{document}
\begin{frontmatter}

\title{Active Nonuniform Sampling}

\author{Bradley Worley\corref{cor1}}
\ead{geekysuavo@gmail.com}

\affiliation{
 organization={Numerion Labs},
 %addressline={250 Sutter},
 city={San Francisco},
 %postcode={94108},
 state={CA},
 country={USA}
}

% ============================================================================
\begin{abstract}
Nonuniform sampling (NUS) is an effective strategy for reducing experiment
times in multidimensional NMR, enabling practitioners to focus instrument
resources on indirect dimension times that carry maximal information
about the measured signal. Given a suitably chosen sampling schedule,
NUS experiments can yield higher spectral quality in less time, and while
many suitable choices of schedule exist for any given experiment, their
selection is still largely based on rules of thumb. This work examines
the desiderata of \emph{active} nonuniform sampling (ANS), which places a
reconstruction algorithm within the acquisition loop to select new samples
on the fly, and develops a candidate ANS algorithm in order to assess its
performance and practicality in two-dimensional experiments.
\end{abstract}

\begin{graphicalabstract}
\includegraphics{example-image-a}
\end{graphicalabstract}

\begin{highlights}
 \item Variationally reweighted least squares (VRLS) reconstructs
 non-uniformly sampled NMR signals by approximating Bayesian
 posterior predictive distributions.

 \item Active nonuniform sampling (ANS) with VRLS adaptively selects
 optimal nonuniform sampling schedules during acquisition.

 \item ANS selects schedules that minimize VRLS reconstruction
 uncertainty.

 \item Optimal nonuniform sampling schedules depend non-trivially on
 the signal being measured.
\end{highlights}

\begin{keyword}
Active learning \sep
Compressed sensing \sep
Nonuniform sampling \sep
Statistical inference \sep
Variational inference
\end{keyword}

\end{frontmatter}
% \linenumbers

% ============================================================================
\section{Introduction}
\label{sec:intro}
Acquiring high-resolution multidimensional NMR spectra is a crucial yet
highly challenging component of a multitude of modern experimental designs.
Nonuniform sampling (NUS) is a common and powerful strategy
\cite{kazimierczuk:jmr2009,rovnyak:mrc2011,hyberts:jbnmr2013,palmer:jbnmr2014}
to reduce total acquisition time while retaining spectral quality by
measuring part of the signal and computationally estimating the rest
\cite{bretthorst:cmr2008}.
However, NUS is not without its own tradeoffs; the ``reconstruction''
algorithm used to estimate unobserved time-domain data is often non-linear
and may artificially suppress weak signal content, introduce spurious
signal content, or narrow spectral lines \cite{maciejewski:jmr2009}.
Because the quality of reconstruction is sensitive to the choice
of which part of the signal is observed (the ``sampling schedule''), much
effort has been invested in designing sampling schedules that perform well
across the broadest possible range of signals
\cite{hyberts:jacs2010,mobli:jmr2015,worley:jmr2015,worley:jmr2016b,%
cullen:mrc2023,love:jmro2025}.

Unfortunately at present, it would appear that no \emph{a priori} method
exists for selecting an optimal sampling schedule \cite{love:jmro2025}.
The frequencies and amplitudes of artifacts in any given reconstructed NMR
spectrum depend how the schedule interferes with the ground-truth signal: the
act of time-domain subsampling is equivalent to a convolution between the
ground-truth spectrum and the Fourier transform of the schedule, or point
spread function (PSF). For any given signal, the task of reconstruction by
deconvolving the PSF may be helped or hindered by our choice of schedule.
Envelope-matched sampling \cite{schuyler:jbnmr2011} strikes an interesting
balance between total agnosticism and full knowledge of the ground-truth
signal. At the same time, it highlights the central issue of \emph{a priori}
schedule selection: the optimal schedule depends on the signal, and the
signal has yet to be measured.

Active nonuniform sampling (ANS, \cite{worley:cmr2018}) has been proposed as
a principled approach to breaking this dependency cycle. In an ANS
experiment, the choice of sampling schedule is ceded to an algorithm
that progressively refines its estimated reconstruction as data are
observed and proposes grid points to sample next. As ANS aims to maximize
the information gained by each sampled point, proposals are made based on
the current uncertainty of the time-domain signal estimate. This immediately
leads to a set of nontrivial requirements that any ANS algorithm must meet:
\begin{enumerate}
 \item Uncertainty estimates must be computationally inexpensive, as each
  proposal must be ready within the recycle delay.
 \item Uncertainties must be accurate enough for active learning,
  suggesting the use of Bayesian posterior predictive variances.
 \item Running ANS on the same sample multiple times must yield
  quantitatively equivalent results.
\end{enumerate}
As Bayesian methods are not known for speed or determinism
\cite{bretthorst1988,bretthorst:jmr1990a,bretthorst:jmr1990b,%
bretthorst:jmr1990c},
these three requirements are in apparent contradiction. The remainder of this
work uses variational approximation \cite{worley:cmr2018,bishop2007} to build
an algorithm specifically tailored for ANS.

% ============================================================================
\section{Theory}
\label{sec:theory}
This work considers the most common special case of NUS, where signals are
acquired in complete quadrature on a uniform (Nyquist) sampling grid and
direct-dimension free induction decays are observed for a subset of the
indirect-dimension grid points based on a sampling schedule.
Without loss of generality, the following equations
will focus on the reconstruction of a single indirect-dimension slice. It is
worth mentioning that the established convention of processing acquired
direct-dimension traces followed by independent reconstructions along the
remaining dimensions effectively ignores nontrivial correlations in the data.
However, this practice is necessary to meet the speed requirement of ANS.

\subsection{Probabilistic construction}
\label{subsec:bayes}
We begin with the standard fixed-basis \cite{worley:cmr2018}
measurement model,
\begin{equation}
\label{eq:signalmodel}
\boldsymbol{y} = \m{A} \m{x} + \m{\epsilon}
\end{equation}
where $\m{y}$ are the observed time-domain data, $\m{x}$ is the spectrum
to be estimated, and $\m{\epsilon}$ is independently and identically
normally distributed noise with zero mean and precision $\tau$. The
$m \times n$ matrix $\m{A} = \m{B} \m{\Phi}$ is composed of a subsampling
matrix $\m{B}$ and a discrete Fourier transform (DFT) matrix $\m{\Phi}$.
Putting these together yields the likelihood,
\begin{equation}
p(\m{y} \mid \m{x}, \tau) =
 (2 \pi)^{-\frac{m}{2}} \tau^{\frac{m}{2}} \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\end{equation}
Placing a normal prior distribution on each spectral value,
\begin{equation}
p(x_i \mid w_i) =
 (2 \pi)^{-\frac{1}{2}} w_i^{\frac{1}{2}} \exp\left\{
  -\frac{1}{2} w_i |x_i|^2
 \right\}
\end{equation}
and an inverse-gamma distribution on the precisions $w_i$ of said prior,
\begin{equation}
p(w_i \mid \xi) =
 \frac{\xi}{2} w_i^{-2} \exp\left\{
  -\frac{\xi}{2} w_i^{-1}
 \right\}
\end{equation}
results in a conditional distribution,
\begin{equation}
\begin{aligned}
\label{eq:joint}
p(&\m{y}, \m{x}, \m{w} \mid \xi, \tau) \propto \\
 & \left( {\textstyle\prod}_i w_i^{-\frac{3}{2}} \right)
 \exp\left\{
  -\frac{\tau}{2} \| \m{y} - \m{A} \m{x} \|_2^2
  -\frac{1}{2} {\textstyle\sum}_i \left( w_i |x_i|^2 + \xi w_i^{-1} \right)
 \right\}
\end{aligned}
\end{equation}
which will be of central importance in subsequent derivations. While it does
not appear related to other sparsity-inducing priors, this distribution can
generate a broad range of $\ell_1$ iteratively reweighted least squares
(IRLS, \cite{daubechies:cpam2010,kazimierczuk:jmr2011})
algorithms.\footnote{%
 See the notes in \url{http://github.com/geekysuavo/irls-sandbox}
 for a more complete overview of $\ell_1$-IRLS algorithms derived
 from this joint distribution.}

Continuing with a fully Bayesian treatment yields diminishing returns beyond
this point, so we employ variational inference \cite{wainwright:ftml2008} to
approximate the posterior over $\m{x}$ and $\m{w}$ by a distribution of the
form $q(\m{x}, \m{w}) = q_x(\m{x}) \, q_w(\m{w})$ that minimizes the
variational free energy,
\begin{equation}
F(q) =
 -\mathbb{E}_{q(\m{x},\m{w})}[\ln p(\m{y}, \m{x}, \m{w}, \mid \xi, \tau)]
 -\mathbb{H}[q(\m{x},\m{w})]
\end{equation}
Noting that the minimizing factors $q_x$ and $q_w$ are a multivariate
normal distribution and a product of inverse normal distributions,
respectively,
\begin{align}
q_x(\m{x}) &= \mathcal{N}(\m{\mu}, \m{\Gamma})
\\
q_w(w_i) &= \mathcal{IN}(\nu_i, \lambda_i)
\end{align}
we rewrite the functional $F(q)$ as a function of the nontrivial
parameters of $q$, namely $\m{\eta} = \{ \m{\mu}, \m{\Gamma}, \m{\nu} \}$,
\begin{equation}
\begin{aligned}
\label{eq:objective}
F(\m{\eta}) &=
 \frac{\tau}{2} \| \m{y} - \m{A} \m{\mu} \|_2^2
+\frac{\tau}{2} \mathrm{tr}(\m{A}^\top \m{A} \m{\Gamma})
-\frac{1}{2} \ln\det\m{\Gamma}
\\ &
+\frac{1}{2} {\textstyle\sum}_i \nu_i (|\mu_i|^2 + \Gamma_{ii})
+\frac{\xi}{2} {\textstyle\sum}_i \nu_i^{-1}
\end{aligned}
\end{equation}

The decision to leave $q_x$ intact in the variational family is intentional.
A crucial component of ANS is the posterior predictive covariance of the
complete signal $\m{\hat y}$, which is equal to
$\m{\Phi} \m{\Gamma} \m{\Phi}^\top$. Had we sought a fully mean-field
approximation in which $\m{\Gamma}$ is diagonal, the matrix
$\m{\Phi} \m{\Gamma} \m{\Phi}^\top$ would be circulant, and therefore
useless in the task of identifying maximally informative grid points.

\subsection{Variationally reweighted least squares}
\label{subsec:vrls}
The VRLS algorithm is obtained by directly applying alternating minimization
to $F(\m{\eta})$. The simplest of these updates is that of the weights
$\m{\nu}$, which is obtained by setting each
$\partial_{\nu_i} F(\m{\eta}) = 0$ and solving for $\nu_i$,
\begin{equation}
\nu_i \gets \sqrt{\xi (|\mu_i|^2 + \Gamma_{ii})^{-1}}
\end{equation}
This mirrors the $\ell_1$-IRLS update of $w_i \gets |x_i|^{-1}$ with
several interesting advantages. In IRLS, a small correction term $c$ is
often added to the update to obtain $w_i \gets (x_i^2 + c)^{-1/2}$ in order
to avoid singularities as $x_i \to 0$ \cite{daubechies:cpam2010}. VRLS
elegantly uses the marginal variance $\Gamma_{ii}$ as an adaptive
correction.

Naively updating either $\m{\mu}$ or $\m{\Gamma}$ in the same fashion
will result in expressions involving dense $n \times n$ matrix inverses,
\begin{equation}
\begin{aligned}
\m{\Gamma} &\gets (\m{V} + \tau \m{A}^\top \m{A})^{-1}
\\
\m{\mu} &\gets \tau \m{\Gamma} \m{A}^\top \m{y}
\end{aligned}
\end{equation}
where $\m{V}$ is a diagonal matrix containing the weights $\m{\nu}$. These
updates are poorly suited to our application, so we will ``kernelize'' them
using the matrix inversion lemma \cite{boyd2004},
\begin{align}
\m{C} &= \m{\Phi} \m{V}^{-1} \m{\Phi}^\top
\\
\m{K} &= \tau^{-1} \m{I} + \m{B} \m{C} \m{B}^\top
\\
\m{\Gamma} &= \m{V}^{-1} - \m{V}^{-1} \m{A}^\top \m{K}^{-1} \m{A} \m{V}^{-1}
\end{align}
where $\m{K}$ is a much smaller $m \times m$ kernel matrix, and $\m{C}$
is a circulant $n \times n$ matrix whose elements may be computed using a
fast Fourier transform (FFT). With these definitions in hand, the mean
update becomes,
\begin{equation}
\m{\mu} \gets \m{V}^{-1} \m{A}^\top \m{K}^{-1} \m{y}
\end{equation}
which no longer requires the full covariance matrix $\m{\Gamma}$. In fact,
the updates to $\m{\nu}$ only require the diagonal elements of $\m{\Gamma}$,
and the posterior predictive variances of $\m{\hat y}$ only require the
diagonal elements of $\m{\Phi} \m{\Gamma} \m{\Phi}^\top$. Both of these
benefit from kernelization, requiring only $\mathcal{O}(m^2 n)$ operations.
Detailed derivations of these computations are provided in the
\nameref{app:si}.

To summarize, VRLS is a globally convergent algorithm that minimizes the
convex objective function \eqref{eq:objective}, with a per-iteration cost
of $\mathcal{O}(n \log n + m^2 n + m^3)$ operations.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-1.pdf}
\caption{%
 (a) $^1$H--$^{15}$N HSQC spectrum reconstructed from 4\%-sparse
 measurements using VRLS. (b) One-dimensional slice taken from the
 reconstruction in (a) overlaid with the square root of its
 accompanying variance estimate.
}
\label{fig:hsqc}
\end{figure*}

\subsection{Extensions to VRLS}
\label{subsec:extensions}
While VRLS only requires handling a small $m \times m$ kernel matrix, it
is nevertheless matrix-bound. A matrix-free fast mean-field variant of
VRLS may be easily constructed using majorize-minimization techniques
\cite{worley:ieeetsp2019}. Because it forms a mean-field approximation
of the posterior, it cannot be used in ANS. Even so, we derive the fast
mean-field updates to $\m{\mu}$ and $\Gamma_{ii}$ in the \nameref{app:si}
to analyze its performance relative to full VRLS.

As written, the VRLS algorithm depends on \emph{a priori} knowledge of the
noise precision $\tau$ and weight parameter $\xi$. While $\tau$ may be
estimated from a noise sample, and empirical results strongly suggest that
tying $\xi=\tau$ yields the best estimates, the two parameters balance the
data fit and sparsity terms in \eqref{eq:joint}, and may require manual
tuning. Alternatively, we may place inverse-gamma priors on $\tau$ and $\xi$
and extend VRLS to learn approximate posterior factors $q_\tau$ and $q_\xi$.
The extended VRLS objective and updates are derived in the \nameref{app:si}.

\subsection{Active sampling}
\label{subsec:ans}
To complete our ANS algorithm, we use VRLS to select time-domain grid points
that have maximum expected information gain \cite{mackay:nc1992}. In our
variational Bayesian framework, this is equivalent to selecting the grid
points whose posterior predictive variance is largest. The complete
time-domain signal $\m{\hat y} = \m{\Phi} \m{x}$ has the following
mean and covariance:
\begin{align}
\mathbb{E}_{q(\m{x},\m{w})}[\m{\hat y}] &=
 \m{\Phi} \m{\mu}
\\
\mathrm{Cov}_{q(\m{x},\m{w})}[\m{\hat y}, \m{\hat y}] &=
 \m{\Phi} \m{\Gamma} \m{\Phi}^\top
\end{align}
so we compute the marginal variances and select a new grid point $i_*$
\begin{equation}
i_* = \underset{i \in [n]}{\arg\max}
 \left( \m{e}_i^\top \m{\Phi} \m{\Gamma} \m{\Phi}^\top \m{e}_i \right)
\end{equation}
for observation. Ideally, this process occurs within the recycle delay of
the experiment and adds no additional overhead to the total acquisition
time.

While VRLS was derived from a probabilistic perspective, it is ultimately
an $\ell_1$-IRLS algorithm subject to compressed sensing (CS) theory
\cite{donoho:ieeeit2006}. As a consequence, ANS must contend with a sort
of ``cold start'' problem: naively beginning with a single measurement
on a large grid places VRLS outside the desirable region of its
sparsity-undersampling phase diagram \cite{donoho:pnas2009} where
successful reconstructions are expected. To overcome this, ANS begins
with an initial number of measurements $m_0 > 1$ uniformly sampled, and
gradually increases the size of its grid $n$ using a CS-inspired heuristic,
\begin{equation}
\label{eq:gridsize}
n_* = \sup\left\{
  n \le n_{\max}
 \mid
  m/n > \varepsilon
 \right\}
\end{equation}
where $\varepsilon \in (0,1)$ is a rough estimate of the sparsity of
the signal, i.e.~the fraction of nonzero elements of $\m{x}$.

% ============================================================================
\section{Materials and methods}
\label{sec:methods}

\subsection{Datasets}
\label{subsec:datasets}
To evaluate the performance of VRLS on realistic NUS data, a
two-dimensional gradient-enhanced $^1$H--$^{15}$N HSQC dataset
that was initially described in \cite{worley:jmr2016a} was reconstructed
from 4\%-sparse quadrature measurements ($m=40, n=1024$). One hundred
iterations of VRLS were performed with $\tau$ and $\xi$ fixed to
$1/\sigma^2$, where $\sigma$ was estimated by manual inspection of
the first transient.

Synthetic data were generated to test ANS across a broad variety of
signal parameters. All simulated signals have the form
\begin{equation}
\label{eq:truth}
y(t) = \sum_{c=1}^{n_c} \alpha_c \exp\left\{
  2 \pi (i \omega_c - \rho_c) t + i \theta_c
 \right\} + \epsilon
\end{equation}
with amplitudes $\{\alpha_c\}$ drawn from a scaled chi-squared distribution
having $\delta_\alpha$ degrees of freedom and mean $\bar\alpha$,
frequencies $\{\omega_c\}$ drawn from a uniform distribution spanning
the entire spectral width, decay rates $\{\rho_c\}$ drawn from a scaled
chi-squared distribution with $\delta_\rho$ degrees of freedom and mean
$\bar\rho$, and phase shifts $\{\theta_c\}$ drawn from a normal
distribution with zero mean and standard deviation $s_\theta$.
The noise $\epsilon$ is drawn from a normal distribution with standard
deviation $\sigma$.

ANS experiments were simulated using synthetic data generated according
to equation \eqref{eq:truth} across a range of signal parameters
(cf.~\nameref{app:si}). In all simulations, one hundred iterations of
VRLS were run after each acquired time point with $\tau$ and $\xi$ fixed
to $1/\sigma^2$. All reconstructions used a time-domain grid of 1,024
points, and one eighth of the total acquisition budget was used to
uniformly sample an initial set of grid points near $t=0$,
i.e.~$m_0 = m_{\mathrm{final}}/8$
where $m_{\mathrm{final}} = \varepsilon n_{\max}$. Runtimes and
spectral reconstruction mean-squared errors (MSE) of the form
\begin{equation}
L(\m{\mu}; \m{x}) = n_{\max}^{-1} \| \m{x} - \m{\mu} \|_2^2
\end{equation}
were computed for each ANS experiment. For each simulated ANS experiment,
a static NUS experiment was simulated using the same signal parameters
and acquisition budget. For simplicity, all static NUS schedules were
drawn from a uniform distribution over grid points.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figure-2.pdf}
\caption{%
 On-GPU runtimes of VRLS ($n_{\mathrm{iter}}=100$, black) and
 FMF-VRLS ($n_{\mathrm{iter}}=1000$, red) as a function of measurement
 count $m$, grid point count $n$, and number of parallel reconstructions
 $p$. Times are reported on a logarithmic scale.
}
\label{fig:time}
\end{figure}

\subsection{Software implementation}
\label{subsec:impl}
A PyTorch \cite{pytorch} extension module was written in C++14 that
implements the VRLS, extended VRLS, and fast mean-field VRLS algorithms.
Computations of the kernel matrix, frequency-domain marginal variances,
and time-domain marginal variances are performed in parallel using the
OpenMP API \cite{dagum:cse1998} on the CPU and using
CUDA \cite{nickolls:siggraph2008} on the GPU. Accelerating computations
on the GPU is important for minimizing the time required to propose new
grid points for sampling, and PyTorch provides many highly optimized
linear algebra and FFT operators for the GPU, making it an ideal
framework for prototyping an ANS algorithm.

\subsection{Software benchmarking}
\label{subsec:bench}
All benchmarks and analyses were conducted on a 2.6 GHz Intel Xeon
8000-series 32-core machine with 226 GiB RAM and a single NVIDIA A10 GPU
(24 GB). The differing runtimes and convergence rates of standard and
mean-field VRLS were compared on simulated signals to assess suitability
for ANS, and full ANS experiments were simulated in order to analyze the
resulting schedules and determine the conditions in which successful ANS
acquisition is feasible.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figure-3.pdf}
\caption{%
 Relative mean-squared error reductions
 $(L_{\mathrm{NUS}}-L_{\mathrm{ANS}})/L_{\mathrm{NUS}}$ in the
 noise-only regions of spectral reconstructions produced from
 static NUS and ANS simulations of signals with randomly
 generated parameters.
}
\label{fig:errors}
\end{figure}

% ============================================================================
\section{Results}
\label{sec:results}

\subsection{Reconstruction quality and runtime}
\label{subsec:vrlsresults}
As illustrated by Figure \ref{fig:hsqc}, VRLS is a capable NUS
reconstruction method on its own, given data acquired from a fixed
schedule. While VRLS provides a point estimate of the spectrum in
$\m{\mu}$, it also provides a variance $\Gamma_{ii}$ with each spectral
point $\mu_i$. Because $q_x(\m{x})$ is an approximation of the intractable
exact posterior $p(\m{x} \mid \m{y}, \xi, \tau)$, the variances from VRLS
tend to under-estimate the true posterior variances
\cite{wainwright:ftml2008}. Nevertheless, they are useful for
distinguishing truly recovered signals from artifacts.

When posterior variance estimates are not required, fast mean-field
(FMF) VRLS can provide mean estimates $\m{\mu}$ of equal quality to
VRLS without requiring the storage or inversion of matrices. However,
the matrix-free advantage of FMF-VRLS comes with disadvantages. Because
FMF-VRLS neglects posterior correlations between spectral points, it
dramatically under-estimates frequency domain posterior variance,
and completely fails to usefully estimate time-domain posterior variance.
Moreover, it often requires an order of magnitude more iterations than
VRLS to converge. Figure \ref{fig:time} summarizes this effect: for
sufficiently small $m$ (high undersampling), VRLS requires less time than
FMF-VRLS to converge to an equivalent or better mean estimate.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figure-4.pdf}
\caption{%
 Representative reconstructions produced from static NUS and ANS
 simulations of signals with randomly generated parameters. From
 front to back, traces are the noisy ground truth signal, the ANS
 reconstruction, and a static NUS reconstruction with the same
 acquisition budget (12\% undersampling).
}
\label{fig:ans}
\end{figure}

\subsection{Active nonuniform sampling simulations}
\label{subsec:ansresults}
Comparing the reconstruction mean-squared errors $L(\m{\mu}; \m{x})$
from ANS and static NUS surprisingly reveals equivalent performance.
However, comparing their reconstruction errors in near-resonance and
noise-only spectral regions shows diverging behaviors
(Figures \ref{fig:errors}, \ref{fig:ans}). Near true resonances, ANS
errors are the same or slightly increased relative to static NUS,
suggesting a tendency of ANS to concentrate any artifacts it produces
around real spectral content. In noise-only regions, ANS errors are
dramatically reduced relative to static NUS, which tends to produce
sharper artifacts more randomly within the spectral window. These trends
are mirrored in the posterior variance estimates from ANS and static NUS,
demonstrating that ANS preferentially selects grid points that inform
VRLS about where the signal \emph{is not present} in the spectrum, as
opposed to where it is. Viewed from the lens of solving the highly
underdetermined linear system \eqref{eq:signalmodel}, such a
``process of elimination'' is not an altogether unreasonable strategy.

Perhaps less surprisingly, the degree to which ANS reduces the error
of noise-only spectral regions, relative to static NUS, depends on
the measurement noise $\sigma$ and the final undersampling rate $m/n$
(Figure \ref{fig:errors}). When acquiring noisy data, the information
gained for each new grid point is lower, reducing the advantage of ANS.
Likewise, in less aggressively undersampled experiments, the variability
of reconstructions due to the choice of schedule is diminished. By
leveraging the uncertainty estimates and variational free energies
computed by VRLS to form an early stopping criterion, we may therefore
limit ANS to this ideal undersampling regime of $m \ll n$ while
controlling reconstruction error.

Repeated simulations of ANS using the same ground truth signal with
different samples of noise $\epsilon$ show that resulting schedules
can vary substantially for subtly different measurements
(Figure \ref{fig:sched}). In other words, while VRLS is a
deterministic and globally convergent algorithm, the variances it
produces remain sensitive to measurement noise. This corroborates the
observation in \cite{schuyler:jbnmr2011} that reconstructions from
beat-matched sampling were less robust than envelope-matched sampling,
despite using more \emph{a priori} information. Even still, overall
trends may be observed in the PSFs of schedules produced by repeated
runs of ANS. Figure \ref{fig:psf} shows that ANS systematically
minimizes the PSF of its schedules in a data-dependent manner, an
effect also observed with a different variational method in
\cite{worley:cmr2018}. The size of this effect depends on measurement
noise, with average PSFs from ANS simulations of noisier signals
exhibiting less pronounced, broader minima.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figure-5.pdf}
\caption{%
 Distribution of sampling schedules produced from 1000 repeated ANS
 simulations of the same noisy signal ($\sigma=0.01$). The gap in
 sampled grid points in the middle of the grid is a consequence of
 the grid size heuristic, which uses power-of-two grid sizes.
}
\label{fig:sched}
\end{figure}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{figure-6.pdf}
\caption{%
 Mean and standard deviations ($\mu\pm 3\sigma$) of point spread
 functions for sampling schedules produced from 1000 repeated ANS
 simulations of the same noisy signal at (a) $\sigma=0.01$ and
 (b) $\sigma=0.5$ arb.~units.
}
\label{fig:psf}
\end{figure*}

% ============================================================================
\section{Conclusions}
\label{sec:concl}
The development of VRLS in this work takes active nonuniform sampling from
the slow, infeasible theoretical possibility presented in
\cite{worley:cmr2018} and progresses it into a practical methodology.
By carefully constructing a joint density \eqref{eq:joint} that
marginalizes to a Laplace prior on $\m{x}$, VRLS avoids complexities
faced by non-convex alternative algorithms like sparse Bayesian learning
(SBL, \cite{worley:ieeetsp2019}). By retaining just enough structure
in $q(\m{x}, \m{w})$ and leveraging the partial Fourier nature of the
measurement matrix $\m{A}$, VRLS efficiently estimates marginal variances
that enable post-reconstruction quality checking and active sampling.
VRLS is well-suited to GPU acceleration due to the embarrassingly parallel
nature of marginal variance estimation; planned optimizations to the CUDA
implementation are expected to further improve runtime scaling with $m$.

Certain characteristics of ANS still warrant further examination.
For example, refinement of the grid size heuristic \eqref{eq:gridsize}
could nudge the algorithm towards producing sampling schedules with more
homogenous time-domain coverage, even though it remains to be seen whether
this is more than an aesthetic trait. Perhaps most obviously, further work
is needed to extend VRLS to multidimensional data where the advantages of
NUS are greater. Doing so is largely an engineering exercise that will
require careful indexing and multi-complex FFTs, as nothing in VRLS
precludes its generalization to higher dimensionalities.

To the author's knowledge, VRLS is the first algorithm to satisfy the
desiderata of ANS and consequently enable time-efficient model-based
active learning on NMR spectrometers.\footnote{%
 Aside from the variational Bayesian approach to ANS presented in
 \cite{worley:cmr2018}, other authors have attempted ANS using the
 sample variance of diffusion models \cite{goffinet:arxiv2025}.
 However, the models in \cite{goffinet:arxiv2025} require minutes to
 propose new grid points and suffer from all the shortcomings of deep
 learning (black box inscrutability, unclear out-of-domain generalization,
 inference-time non-determinism, etc.).
} However, it remains limited in
many important aspects. Principal among these limitations its its continued
use of a fixed-basis signal model \eqref{eq:signalmodel}, which discards
nearly all available prior information about the ground truth signal in
exchange for the use of a fast Fourier transform and a downstream cascade
of signal processing, peak picking, and so on. Prior knowledge of
experimental parameters, the number of resonances and their relationships
in protein samples, or the contributions of possible analytes in complex
mixtures, are all prime candidates for Bayesian modeling. While such
parametric signal models are currently too slow for ANS, they promise
the dual advantages of (i) further reducing acquisition time by leveraging
prior information and (ii) providing spectroscopists with richer and more
immediate experimental results. Indeed, further work is needed to develop
Bayesian NMR signal models that make full use of available prior knowledge
while also admitting active learning.

% ============================================================================
\appendix
\section{Supplementary information}
\label{app:si}
This material is available free of charge via the Internet at
\url{http://www.sciencedirect.com}. Supplementary data associated
with this article can be found, in the online version, at
\url{http://dx.doi.org/10.1016/j.jmr.0000.00.000}

\section*{Data availability}
\label{app:data}
The data and code used to produce this work can be found online at
\url{http://github.com/geekysuavo/vrls-nmr}.

% ============================================================================
\bibliographystyle{elsarticle-num}
\bibliography{vrls}

\end{document}
\endinput
